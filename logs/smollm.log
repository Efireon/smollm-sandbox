[2025-03-07 15:13:06.626] [INFO] [main.go:48] Starting SmolLM Sandbox
[2025-03-07 15:13:06.626] [INFO] [main.go:51] Loading configuration from configs/config.yaml
[2025-03-07 15:13:06.627] [INFO] [main.go:59] Initializing SmolLM2 model
[2025-03-07 15:13:06.627] [INFO] [smollm.go:45] Initializing SmolLM2 model
[2025-03-07 15:13:06.627] [INFO] [main.go:63] Setting up sandbox environment
[2025-03-07 15:13:06.627] [INFO] [environment.go:52] Initializing sandbox environment
[2025-03-07 15:13:12.314] [INFO] [smollm.go:203] Mock inference with input length: 30
[2025-03-07 15:13:17.582] [INFO] [smollm.go:203] Mock inference with input length: 213
[2025-03-07 15:13:26.050] [INFO] [smollm.go:203] Mock inference with input length: 405
[2025-03-07 15:13:38.305] [INFO] [smollm.go:203] Mock inference with input length: 583
[2025-03-07 15:13:39.389] [INFO] [smollm.go:203] Mock inference with input length: 768
[2025-03-07 15:13:40.657] [INFO] [smollm.go:203] Mock inference with input length: 952
[2025-03-07 15:13:55.991] [INFO] [smollm.go:203] Mock inference with input length: 992
[2025-03-07 15:14:51.671] [INFO] [smollm.go:203] Mock inference with input length: 1035
[2025-03-07 15:14:52.206] [INFO] [smollm.go:203] Mock inference with input length: 1031
[2025-03-07 15:15:38.936] [INFO] [smollm.go:203] Mock inference with input length: 1005
[2025-03-07 15:15:44.679] [INFO] [smollm.go:203] Mock inference with input length: 1001
[2025-03-07 15:15:51.836] [INFO] [smollm.go:203] Mock inference with input length: 930
[2025-03-07 15:15:53.266] [INFO] [smollm.go:203] Mock inference with input length: 883
[2025-03-07 15:16:02.582] [INFO] [main.go:83] SmolLM Sandbox finished
[2025-03-07 15:30:04.173] [INFO] [main.go:52] Starting SmolLM Sandbox v0.1.0
[2025-03-07 15:30:04.173] [INFO] [main.go:55] Loading configuration from configs/config.yaml
[2025-03-07 15:30:04.173] [INFO] [main.go:63] Initializing SmolLM2 model
[2025-03-07 15:30:04.173] [INFO] [smollm.go:46] Initializing SmolLM2 model
[2025-03-07 15:30:04.173] [INFO] [inference.go:83] Starting model API server...
[2025-03-07 15:31:16.886] [INFO] [main.go:52] Starting SmolLM Sandbox v0.1.0
[2025-03-07 15:31:16.886] [INFO] [main.go:55] Loading configuration from configs/config.yaml
[2025-03-07 15:31:16.886] [INFO] [main.go:63] Initializing SmolLM2 model
[2025-03-07 15:31:16.886] [INFO] [smollm.go:46] Initializing SmolLM2 model
[2025-03-07 15:31:16.886] [INFO] [inference.go:83] Starting model API server...
[2025-03-07 15:36:40.873] [ERROR] [inference.go:72] Failed to start model server: ошибка установки зависимостей: exit status 1
[2025-03-07 15:36:40.873] [INFO] [main.go:67] Setting up sandbox environment
[2025-03-07 15:36:40.873] [INFO] [environment.go:52] Initializing sandbox environment
[2025-03-07 15:36:58.709] [INFO] [inference.go:344] Local inference for prompt length: 299
[2025-03-07 15:37:09.087] [ERROR] [smollm.go:89] Inference error: ошибка установки зависимостей: exit status 1
[2025-03-07 15:37:36.523] [INFO] [inference.go:344] Local inference for prompt length: 501
[2025-03-07 15:37:36.566] [ERROR] [smollm.go:89] Inference error: ошибка выполнения скрипта: exit status 1, stderr: Traceback (most recent call last):
  File "/tmp/smollm_inference/inference.py", line 2, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

[2025-03-07 15:39:09.805] [INFO] [inference.go:344] Local inference for prompt length: 693
[2025-03-07 15:39:09.829] [ERROR] [smollm.go:89] Inference error: ошибка выполнения скрипта: exit status 1, stderr: Traceback (most recent call last):
  File "/tmp/smollm_inference/inference.py", line 2, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

